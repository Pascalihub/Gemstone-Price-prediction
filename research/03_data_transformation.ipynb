{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\PASCAL\\\\Gemstone-Price-prediction\\\\research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\PASCAL\\\\Gemstone-Price-prediction'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    preprocessor_path: Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gemstone.constants import *\n",
    "from src.gemstone.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            preprocessor_path= config.preprocessor_path \n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.gemstone import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder,StandardScaler\n",
    "\n",
    "from src.gemstone.exception import CustomException\n",
    "from src.gemstone.logger import logging\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from src.gemstone.utils.common import save_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataTransformation:\n",
    "    # Define categorical_columns as a class-level attribute\n",
    "    categorical_columns = ['cut', 'color', 'clarity']\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def get_data_transformation_object(self):\n",
    "        '''\n",
    "        This function is responsible for data transformation\n",
    "        '''\n",
    "        try:\n",
    "            # Define which columns should be ordinal-encoded and which should be scaled\n",
    "            categorical_cols = ['cut', 'color', 'clarity']\n",
    "            numerical_cols = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
    "\n",
    "            # Define the custom ranking for each ordinal variable\n",
    "            cut_categories = ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal']\n",
    "            color_categories = ['D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "            clarity_categories = ['I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF']\n",
    "\n",
    "            # Numerical Pipeline\n",
    "            num_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='median')),\n",
    "                    ('scaler', StandardScaler())\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Categorical Pipeline\n",
    "            cat_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('ordinal_encoder', OrdinalEncoder(categories=[cut_categories, color_categories, clarity_categories])),\n",
    "                    ('scaler', StandardScaler())\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            logging.info(f'Categorical Columns : {categorical_cols}')\n",
    "            logging.info(f'Numerical Columns   : {numerical_cols}')\n",
    "\n",
    "            preprocessor = ColumnTransformer(\n",
    "                [\n",
    "                    ('num_pipeline', num_pipeline, numerical_cols),\n",
    "                    ('cat_pipeline', cat_pipeline, categorical_cols)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            return preprocessor\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info('Exception occurred in Data Transformation Phase')\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def initiate_data_transformation(self):\n",
    "        try:\n",
    "            # Corrected path construction\n",
    "            data_path = os.path.join(\"artifacts\", \"data_ingestion\", \"gemstone.csv\")\n",
    "\n",
    "            # Reading data\n",
    "            data = pd.read_csv(data_path)\n",
    "\n",
    "            # Split the data into training and test sets. (0.75, 0.25) split.\n",
    "            train, test = train_test_split(data, test_size=0.25, random_state=42)\n",
    "\n",
    "            logging.info('Obtaining preprocessing object')\n",
    "            preprocessing_obj = self.get_data_transformation_object()\n",
    "\n",
    "            target_column_name = 'price'\n",
    "            drop_columns = [target_column_name, 'id']\n",
    "\n",
    "            input_feature_train_df = train.drop(columns=drop_columns, axis=1)\n",
    "            target_feature_train_df = train[target_column_name]\n",
    "\n",
    "            input_feature_test_df = test.drop(columns=drop_columns, axis=1)\n",
    "            target_feature_test_df = test[target_column_name]\n",
    "\n",
    "            logging.info(\"Applying preprocessing object on training and testing datasets.\")\n",
    "\n",
    "            input_feature_train_arr = preprocessing_obj.fit_transform(input_feature_train_df)\n",
    "            input_feature_test_arr = preprocessing_obj.transform(input_feature_test_df)\n",
    "\n",
    "            train_arr = np.c_[input_feature_train_arr, np.array(target_feature_train_df)]\n",
    "            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]\n",
    "\n",
    "            # Convert NumPy arrays to DataFrames\n",
    "            train_df = pd.DataFrame(train_arr, columns=list(input_feature_train_df.columns) + [target_column_name])\n",
    "            test_df = pd.DataFrame(test_arr, columns=list(input_feature_test_df.columns) + [target_column_name])\n",
    "\n",
    "            # Save train and test data to CSV\n",
    "            train_df.to_csv(os.path.join(self.config.root_dir, \"train.csv\"), index=False)\n",
    "            test_df.to_csv(os.path.join(self.config.root_dir, \"test.csv\"), index=False)\n",
    "\n",
    "\n",
    "            # Save preprocessing object\n",
    "            preprocessing_obj_file = os.path.join(\"artifacts\", 'data_transformation', 'preprocessing_obj.pkl')\n",
    "            with open(preprocessing_obj_file, 'wb') as file:\n",
    "                pickle.dump(preprocessing_obj, file)\n",
    "\n",
    "            logging.info(\"Saved preprocessing object.\")\n",
    "            logging.info(\"Transformation of the data is completed\")\n",
    "\n",
    "            return train_arr, test_arr, preprocessing_obj_file\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info('Exception occurred in initiate_data_transformation function')\n",
    "            raise CustomException(e, sys)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.initiate_data_transformation()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PASCAL\\anaconda3\\envs\\gemstone\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SimpleImputer from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\PASCAL\\anaconda3\\envs\\gemstone\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\PASCAL\\anaconda3\\envs\\gemstone\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\PASCAL\\anaconda3\\envs\\gemstone\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator OrdinalEncoder from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\PASCAL\\anaconda3\\envs\\gemstone\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator ColumnTransformer from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the saved preprocessing object from the pickle file\n",
    "preprocessing_obj_file = \"model/preprocessor.pkl\"  # Update with the correct file path\n",
    "with open(preprocessing_obj_file, 'rb') as file:\n",
    "    loaded_preprocessing_obj = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
